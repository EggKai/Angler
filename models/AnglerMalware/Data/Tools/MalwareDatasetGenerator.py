import os
import requests
import datetime
import csv
import hashlib
import pyzipper
import pefile
import shutil
import stat
import concurrent.futures

def on_rm_error(func, path, exc_info):
    """
    Handle read-only files by removing the read-only flag,
    then retrying removal.
    """
    if not os.access(path, os.W_OK):
        os.chmod(path, stat.S_IWRITE)
        func(path)
    else:
        raise

def get_sha256(file_path):
    """Compute the SHA256 hash of a file on disk."""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            sha256_hash.update(chunk)
    return sha256_hash.hexdigest()

def extract_imports(pe):
    """Extract imported DLL names and function names from a PE file."""
    imports = []
    if hasattr(pe, "DIRECTORY_ENTRY_IMPORT"):
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            dll_name = entry.dll.decode("utf-8", errors="ignore")
            for imp in entry.imports:
                if imp.name:
                    func_name = imp.name.decode("utf-8", errors="ignore")
                else:
                    # Some imports may not have a name (import by ordinal)
                    func_name = f"Ordinal({imp.ordinal})"
                imports.append(f"{dll_name}:{func_name}")
    return imports

def process_exe_file(exe_path, date_str, csv_writer):
    """
    Reads an EXE into memory, parses with pefile, writes results (sha256 + imports)
    to the CSV writer.
    """
    try:
        # Read entire file into memory to avoid file-lock issues
        with open(exe_path, "rb") as f:
            exe_data = f.read()

        # Parse the PE structure from memory
        pe = pefile.PE(data=exe_data)

        # Get the list of imports
        imports = extract_imports(pe)

        # Compute SHA256 of the file on disk
        file_sha256 = get_sha256(exe_path)

        # Write row to CSV
        csv_writer.writerow([date_str, os.path.basename(exe_path), file_sha256, "|".join(imports)])
    except Exception as e:
        print(f"[-] Failed to process {exe_path}: {e}")

def download_and_process_zip(date_str):
    """
    1. Builds a per-day CSV filename: exe_analysis_results_{date_str}.csv
       and removes it if it already exists (avoids duplicates).
    2. Downloads the daily ZIP for date_str (if available).
    3. Extracts it with password='infected'.
    4. Processes all .exe files.
    5. Cleans up the ZIP and extracted folder.
    """
    base_url = "https://datalake.abuse.ch/malware-bazaar/daily/"
    zip_filename = f"{date_str}.zip"
    zip_url = f"{base_url}{zip_filename}"

    # --- Step 0: Remove old CSV if present (avoid duplicates on restart) ---
    csv_file_path = f"exe_analysis_results_{date_str}.csv"
    if os.path.exists(csv_file_path):
        print(f"[!] Removing existing CSV {csv_file_path} to avoid duplicates.")
        os.remove(csv_file_path)

    # --- HEAD request to get file size (optional) ---
    try:
        head_resp = requests.head(zip_url, allow_redirects=True, timeout=15)
        if head_resp.status_code == 200 and "Content-Length" in head_resp.headers:
            file_size_bytes = int(head_resp.headers["Content-Length"])
            file_size_mb = file_size_bytes / (1024 * 1024)
            print(f"[+] File size for {zip_filename}: {file_size_mb:.2f} MB")
        else:
            print(f"[!] Could not determine file size for {zip_url} (status={head_resp.status_code}).")
    except Exception as e:
        print(f"[!] HEAD request failed for {zip_url}: {e}")

    # --- Download ZIP ---
    print(f"[+] Downloading {zip_url} ...")
    try:
        response = requests.get(zip_url, stream=True, timeout=60)
        if response.status_code != 200:
            print(f"[-] Unable to download {zip_url}, status code: {response.status_code}. Skipping.")
            return
    except Exception as e:
        print(f"[-] Exception while downloading {zip_url}: {e}")
        return

    with open(zip_filename, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)

    # --- Extract ZIP ---
    extract_dir = f"extracted_{date_str}"
    if not os.path.exists(extract_dir):
        os.makedirs(extract_dir)

    try:
        with pyzipper.AESZipFile(zip_filename) as zf:
            zf.pwd = b"infected"
            zf.extractall(path=extract_dir)
    except Exception as e:
        print(f"[-] Could not extract {zip_filename} with password='infected'. Error: {e}")
        if os.path.exists(extract_dir):
            shutil.rmtree(extract_dir, onerror=on_rm_error)
        return

    # --- Create CSV and process .exe files ---
    print(f"[+] Creating CSV for {date_str}: {csv_file_path}")
    with open(csv_file_path, "w", newline="", encoding="utf-8") as csvfile:
        csv_writer = csv.writer(csvfile)
        # Write header
        csv_writer.writerow(["Date", "FileName", "SHA256", "Imports"])

        # Walk extracted dir and parse all .exe
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                if file.lower().endswith(".exe"):
                    exe_path = os.path.join(root, file)
                    process_exe_file(exe_path, date_str, csv_writer)

    # --- Cleanup ZIP and extracted folder ---
    if os.path.exists(zip_filename):
        os.remove(zip_filename)
    if os.path.exists(extract_dir):
        shutil.rmtree(extract_dir, onerror=on_rm_error)

def main():
    """
    1. Builds a list of all dates between start_date and end_date.
    2. Processes them in chunks of 10 days.
    3. Uses threading to process up to 5 days in parallel within each chunk.
    4. Each day results in its own CSV file: exe_analysis_results_YYYY-MM-DD.csv
    """
    start_date = datetime.date(2025, 1, 1)
    end_date   = datetime.date(2025, 1, 22)

    # Build list of all days in range
    all_days = []
    current = start_date
    while current <= end_date:
        all_days.append(current)
        current += datetime.timedelta(days=1)

    chunk_size = 10
    max_workers = 5  # Number of parallel downloads/processing

    # Process each chunk
    for i in range(0, len(all_days), chunk_size):
        chunk = all_days[i : i + chunk_size]

        # Use a thread pool to process each day in this chunk in parallel
        print(f"\n[===] Processing days {chunk[0]} through {chunk[-1]} in parallel...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_day = {}
            for day in chunk:
                date_str = day.strftime("%Y-%m-%d")
                print(f"[==] Scheduling day: {date_str}")
                future = executor.submit(download_and_process_zip, date_str)
                future_to_day[future] = date_str

            # Wait for each day to finish
            for future in concurrent.futures.as_completed(future_to_day):
                d_str = future_to_day[future]
                try:
                    future.result()  # Raises any exception from the thread
                    print(f"[+] Finished processing {d_str}")
                except Exception as e:
                    print(f"[!] Error processing {d_str}: {e}")

        # Optional: pause between chunks if desired (e.g., to avoid overloading)
        # import time
        # time.sleep(5)

    print("\n[+] Done! Each day now has a separate CSV: exe_analysis_results_YYYY-MM-DD.csv")

if __name__ == "__main__":
    main()
